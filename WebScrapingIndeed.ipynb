{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c86ec9f",
   "metadata": {},
   "source": [
    "# Web-Scraping-Job-From-Indeed\n",
    "Web scraping job from Indeed using Beautifulsoup and checking top words and specific skills according to the description of the job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2585120e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the libraries\n",
    "import requests\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from textblob import TextBlob\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from operator import itemgetter\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78941873",
   "metadata": {},
   "source": [
    "### Let´s request access to the Indeed website, looking for Data Analyst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6e456992",
   "metadata": {},
   "outputs": [],
   "source": [
    "def request_URL(URL):\n",
    "    #show the website\n",
    "    #print(URL)\n",
    "    \n",
    "    #conducting a request of the stated URL above:\n",
    "    page = requests.get(URL)\n",
    "\n",
    "    #specifying a desired format of 'page' using the html parser - this allows python to read the various components of the page, rather than treating it as one long string.\n",
    "    soup = BeautifulSoup(page.text, 'html.parser')\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8b62eb3d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# we will search for job that contain the word bellow as example\n",
    "word = 'data+analyst'\n",
    "URL = 'https://ie.indeed.com/jobs?as_and=' + word + '&as_phr=&as_any=&as_not=&as_ttl=&as_cmp=&jt=all&st=&salary=&radius=100&l=Dublin&fromage=any&limit=1000&sort=date&psf=advsrch&from=advancedsearch'\n",
    "soup = request_URL(URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922a1c06",
   "metadata": {},
   "source": [
    "### Let´s get the identification of each job to check the full description of the job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d3ca70cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the identification of each job\n",
    "keys = [div.get('data-jk') for div in soup.findAll('div', attrs={'data-tn-component': 'organicJob'})]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a887406b",
   "metadata": {},
   "source": [
    "### Let´s request the access of each specific information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3ee6f894",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# For each specific job(key) get the request\n",
    "job_information = []\n",
    "for job in keys:\n",
    "    URL_job = 'https://ie.indeed.com/viewjob?jk=' + job\n",
    "    soup_job = request_URL(URL_job)\n",
    "   \n",
    "    company = soup_job.find(name='div', attrs={'class':'icl-u-lg-mr--sm icl-u-xs-mr--xs'}).get_text()\n",
    "    title = soup_job.find(name='h1', attrs={'class':'icl-u-xs-mb--xs icl-u-xs-mt--none jobsearch-JobInfoHeader-title'}).get_text()\n",
    "    job_description = soup_job.find('div', attrs={'id': 'jobDescriptionText'}).get_text()\n",
    "    date = datetime.now()    \n",
    "    \n",
    "    job_information.append((date, company, title, job_description, URL_job))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2f12d878",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using set_option to show all text - not truncated\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "df = pd.DataFrame(job_information,columns=['date','company','title','full description', 'link'])\n",
    "total_comp = len(df['company'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276451a8",
   "metadata": {},
   "source": [
    "### Let´s create a file with the information about the jobs selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c65b7578",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 22] Invalid argument: 'Files/jobs_indeed_\"data+analyst\".csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-0048d76f9780>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mfile_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'Files/jobs_indeed_'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mword\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'.csv'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'a'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\Web Scraping\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mto_csv\u001b[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[0;32m   3385\u001b[0m         )\n\u001b[0;32m   3386\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3387\u001b[1;33m         return DataFrameRenderer(formatter).to_csv(\n\u001b[0m\u001b[0;32m   3388\u001b[0m             \u001b[0mpath_or_buf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3389\u001b[0m             \u001b[0mline_terminator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mline_terminator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Web Scraping\\lib\\site-packages\\pandas\\io\\formats\\format.py\u001b[0m in \u001b[0;36mto_csv\u001b[1;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[0;32m   1081\u001b[0m             \u001b[0mformatter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfmt\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1082\u001b[0m         )\n\u001b[1;32m-> 1083\u001b[1;33m         \u001b[0mcsv_formatter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1084\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1085\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcreated_buffer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Web Scraping\\lib\\site-packages\\pandas\\io\\formats\\csvs.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    226\u001b[0m         \"\"\"\n\u001b[0;32m    227\u001b[0m         \u001b[1;31m# apply compression and byte/text conversion\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 228\u001b[1;33m         with get_handle(\n\u001b[0m\u001b[0;32m    229\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    230\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Web Scraping\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    640\u001b[0m                 \u001b[0merrors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"replace\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    641\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 642\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    643\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    644\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: [Errno 22] Invalid argument: 'Files/jobs_indeed_\"data+analyst\".csv'"
     ]
    }
   ],
   "source": [
    "file_name = 'Files/jobs_indeed_' + word + '.csv'\n",
    "df.to_csv(file_name, mode='a', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805deba7",
   "metadata": {},
   "source": [
    "### Now we need to clean up the text to get rid of space, blank lines, punctuation..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c19ea0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# cleaning using regex\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'[\\n]',' ',text) # delete \\n\n",
    "    text = re.sub('\\s+',' ',text) # delete spaces\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation),'',text) # delete all punctuation\n",
    "    text = re.sub('—','',text)\n",
    "    text = re.sub('’s','',text)\n",
    "    text = re.sub('Â·','',text)\n",
    "    text = re.sub('â€“','',text)\n",
    "    text = re.sub('â€™','',text)\n",
    "    text = re.sub('. . .','',text)\n",
    "    text = re.sub('##','',text)\n",
    "    text = re.sub('“','',text)\n",
    "    text = re.sub('”','',text)\n",
    "    text = re.sub('‘','',text)\n",
    "    text = re.sub('’','',text)\n",
    "    return text\n",
    "\n",
    "# cleaning the column summary using regex\n",
    "df['full description'] = df['full description'].apply(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c233d6d5",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Now we will check the frequency of the words and eliminate the irrelevant words(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a78be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "blob = TextBlob(str(df['full description']))\n",
    "\n",
    "# word_counts is a dictionary, so we use items and get rid off the stop words\n",
    "items = blob.word_counts.items() \n",
    "\n",
    "# get rid off stop words\n",
    "stop_words = stopwords.words('english')\n",
    "items = [item for item in items if item[0] not in stop_words]\n",
    "\n",
    "# now we will create a df with the frequency of the words\n",
    "df_word_count = pd.DataFrame(items, columns=['word','count'])\n",
    "df_word_count.sort_values(by='count',ascending=False,inplace=True)\n",
    "print(f'Total of words found without stopwords: {len(df_word_count)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab847e93",
   "metadata": {},
   "source": [
    "### Now let´s analyze what kind of speech we find, if the word is adjetive, verb, noum... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eaed061",
   "metadata": {},
   "outputs": [],
   "source": [
    "def part_speech(text):\n",
    "    tokenized = word_tokenize(text)\n",
    "    classif = [pos for word,pos in pos_tag(tokenized)]\n",
    "    return ' '.join(classif)\n",
    "# apply the categorizing function to the word \n",
    "df_word_count['classif'] = df_word_count['word'].apply(part_speech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa2c221",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the part of speech founded\n",
    "df_word_count['classif'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aae7cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before the visualization, let´s filter some part of speech that are not relevant\n",
    "df_word_count = df_word_count[~df_word_count['classif'].isin(['IN','PRP','RB','MD','CD','DT','RBR','CC','WDT','WP$'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea26b43",
   "metadata": {},
   "source": [
    "### Visualization of the Top words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca3578a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plot the top20 words\n",
    "df_top = df_word_count.head(20)\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('fivethirtyeight')\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.title(f'Top 20 words most found on Indeed website on jobs looking for \"{word}\"\\nTotal Jobs: {total_comp}',fontsize=20)\n",
    "plt.barh(df_top['word'],df_top['count'])\n",
    "plt.yticks(fontsize=18)\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529e1783",
   "metadata": {},
   "source": [
    "### Let´s check some skills that the companies are looking for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d428a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.indeed.com/career-advice/resumes-cover-letters/data-analyst-skills-resume\n",
    "skills = ['python','database','sql','oracle','sybase','tableau', 'power','pentaho','excel', 'aws','git','scrum','agile','statistic','math','report','visualiz','azure','cloud','lifecycle','etl',\n",
    "          'model','decision','collect','clean','present','communication','logic','detail','predictive','warehous','machine','mining']\n",
    "\n",
    "list_skills = []\n",
    "for s in skills:\n",
    "    list_skills.append((s,'/'.join(df['company'][df['full description'].str.lower().str.contains(s)])))\n",
    "    \n",
    "df_skill = pd.DataFrame(list_skills, columns=['skill','companies'])\n",
    "\n",
    "# insert a column with the total of companies looking for the skill\n",
    "df_skill['total'] = df_skill['companies'].str.count('/')\n",
    "df_skill.sort_values(by='total',ascending=False,inplace=True)\n",
    "df_skill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4763e3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the skills per company\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('fivethirtyeight')\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.title(f'SKILLS wanted by Companies that are looking for \"{word}\"\\nSource: Indeed',fontsize=20)\n",
    "plt.barh(df_skill['skill'],df_skill['total'])\n",
    "plt.yticks(fontsize=18)\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f4b491",
   "metadata": {},
   "source": [
    "### Conclusion:\n",
    "Observing the plots above we can see that the companies looking for Data Analyst on Indeed website, \n",
    "Excel and SQL are the most wanted technical skill and\n",
    "Data, business and experience are the most wanted soft skill"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
